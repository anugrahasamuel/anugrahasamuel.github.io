[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an MS Social Data Analytics and Research student at The University of Texas at Dallas.\nI‚Äôm interested in how data, storytelling, and quantitative methods can help us better understand social issues.\nThis site is a place to share:\n\nCourse projects and assignments\n\nExamples of my R and Quarto work\n\nReflections on research and data collection\n\n\n\nYou can view or download my current CV here:\nüìÑ Download my resume"
  },
  {
    "objectID": "about.html#my-resume",
    "href": "about.html#my-resume",
    "title": "About",
    "section": "",
    "text": "You can view or download my current CV here:\nüìÑ Download my resume"
  },
  {
    "objectID": "epps6302_assignment2.html",
    "href": "epps6302_assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "write"
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5 ‚Äì Government Data API",
    "section": "",
    "text": "In this assignment I used the GovInfo search interface to find recent documents related to the Foreign Relations Committee. :contentReferenceoaicite:1\n\nI went to https://www.govinfo.gov/app/search/.\nI searched for Foreign Relations Committee hearings and filtered the results to the most recent Congress.\nFrom the search results page, I used the Download Results option to export the metadata as both CSV and JSON files.\nI saved these files into my Quarto project under the data/ folder.\nUsing the provided govtdata01.R template, I modified the script so that it:\n\nReads the CSV and JSON files from the data/ folder,\nExtracts the pdfLink for each result,\nDownloads the 10 most recent PDFs into the files/ folder,\nPauses briefly between downloads to avoid overwhelming the server. :contentReferenceoaicite:2\n\n\nAfter running the script, the ten PDFs appear under files/, and the _quarto.yml resources: setting ensures that they are copied into the rendered website."
  },
  {
    "objectID": "assignment5.html#scraping-government-documents-from-govinfo",
    "href": "assignment5.html#scraping-government-documents-from-govinfo",
    "title": "Assignment 5 ‚Äì Government Data API",
    "section": "",
    "text": "In this assignment I used the GovInfo search interface to find recent documents related to the Foreign Relations Committee. :contentReferenceoaicite:1\n\nI went to https://www.govinfo.gov/app/search/.\nI searched for Foreign Relations Committee hearings and filtered the results to the most recent Congress.\nFrom the search results page, I used the Download Results option to export the metadata as both CSV and JSON files.\nI saved these files into my Quarto project under the data/ folder.\nUsing the provided govtdata01.R template, I modified the script so that it:\n\nReads the CSV and JSON files from the data/ folder,\nExtracts the pdfLink for each result,\nDownloads the 10 most recent PDFs into the files/ folder,\nPauses briefly between downloads to avoid overwhelming the server. :contentReferenceoaicite:2\n\n\nAfter running the script, the ten PDFs appear under files/, and the _quarto.yml resources: setting ensures that they are copied into the rendered website."
  },
  {
    "objectID": "assignment5.html#difficulties-encountered",
    "href": "assignment5.html#difficulties-encountered",
    "title": "Assignment 5 ‚Äì Government Data API",
    "section": "2. Difficulties encountered",
    "text": "2. Difficulties encountered\n\n2.1 Technical and practical issues\nWhile scraping the government data, I ran into several practical difficulties:\n\nFile paths and working directory\nThe script assumes a specific working directory. If RStudio was not set to the project root, the read.csv() and read_json() calls failed with ‚Äúcannot open file‚Äù errors. I fixed this by explicitly setting the working directory and using relative paths like data/... and files/....\nCSV header rows and formats\nThe CSV export from GovInfo includes explanatory header lines before the actual field names. If I did not skip these rows, the data frame loaded incorrectly. Using skip = 2 (as in the template) made the import work reliably. :contentReferenceoaicite:3\nInconsistent or missing links\nNot every record has a clean pdfLink. In some cases, the JSON had missing or NA values, so the code had to be careful about indexing and only trying to download valid URLs.\nDownload reliability and rate limiting\nDownloading multiple PDFs in a row sometimes led to slow responses or temporary failures. The tryCatch() wrapper and a random Sys.sleep() between 1‚Äì3 seconds helped handle occasional errors gracefully and kept the script from appearing abusive to the server.\n\n\n\n2.2 How usable is the scraped data?\nThe metadata and PDFs from GovInfo are quite usable, but not ‚Äúanalysis-ready‚Äù:\n\nThe JSON metadata provides structured fields such as titles, dates, and committee names, which are very helpful for filtering and organizing hearings.\nHowever, the actual content of the hearings is embedded in PDF documents, which are not directly machine-readable. To analyze the text (for example, topic modeling or sentiment analysis), I would still need to run an additional PDF-to-text extraction step.\nSome records are duplicated across different formats, and their identifiers are not always intuitive, so the user must carefully deduplicate or standardize IDs.\n\nOverall, the scraped data works well for building a corpus of documents, but it requires extra preprocessing before any quantitative analysis.\n\n\n2.3 How the process could be improved\nSeveral improvements could make this workflow smoother and more robust:\n\nUse relative paths and project options\nInstead of hard-coding setwd(\"yourpath\"), it would be better to rely on RStudio Projects or the here package so that the script always starts from the project root, regardless of machine or user.\nFilter and validate links before download\nAdding checks to ensure that pdfLink is not NA and that it starts with https:// would avoid wasted download attempts.\nAutomated logging of errors\nExtending the tryCatch() block to write failed URLs and error messages to a log file would make it easier to re-run only the failed downloads later.\nText extraction and additional data cleaning\nA next step would be to automatically convert the downloaded PDFs to text (using packages like pdftools) and store them as cleaned .txt files. This would make the data much more usable for statistical or text-mining analysis.\nParameterization\nTurning the script into a function where I can pass the committee name, number of documents, and output folder as arguments would make it easier to reuse the code for other committees or topics."
  },
  {
    "objectID": "assignment5.html#summary",
    "href": "assignment5.html#summary",
    "title": "Assignment 5 ‚Äì Government Data API",
    "section": "3. Summary",
    "text": "3. Summary\nThis assignment showed how to move from a search interface on a government website to a reproducible R script that downloads primary documents into a structured project. Setting up the file paths correctly, handling messy CSV/JSON exports, and dealing with download errors were the main challenges. Once those issues were resolved, the process of automatically pulling the ten most recent Foreign Relations Committee documents became straightforward, and I now have a reusable template for future government-data projects."
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2 ‚Äì Google Trends",
    "section": "",
    "text": "library(tidyverse)\n\n# 1.1 Load the CSV you downloaded from Google Trends\nweb_trends &lt;- read_csv(\n  \"data/web_trends_trump_kamala_election.csv\",\n  skip = 2\n)\n\n# 1.2 Inspect structure (optional)\nglimpse(web_trends)\ncolnames(web_trends)\n\n# Rename the date column to `date`, convert to Date,\n# and make all keyword columns numeric (handle \"&lt;1\")\nweb_trends &lt;- web_trends |&gt;\n  rename(date = 1) |&gt;\n  mutate(\n    date = as.Date(date),\n    across(\n      -date,\n      ~ as.numeric(ifelse(as.character(.) == \"&lt;1\", \"0\", as.character(.)))\n    )\n  )\n\n# 1.3 Check date range and interval\nrange(web_trends$date)\nunique(diff(web_trends$date))"
  },
  {
    "objectID": "assignment2.html#website-data-load-and-inspect",
    "href": "assignment2.html#website-data-load-and-inspect",
    "title": "Assignment 2 ‚Äì Google Trends",
    "section": "",
    "text": "library(tidyverse)\n\n# 1.1 Load the CSV you downloaded from Google Trends\nweb_trends &lt;- read_csv(\n  \"data/web_trends_trump_kamala_election.csv\",\n  skip = 2\n)\n\n# 1.2 Inspect structure (optional)\nglimpse(web_trends)\ncolnames(web_trends)\n\n# Rename the date column to `date`, convert to Date,\n# and make all keyword columns numeric (handle \"&lt;1\")\nweb_trends &lt;- web_trends |&gt;\n  rename(date = 1) |&gt;\n  mutate(\n    date = as.Date(date),\n    across(\n      -date,\n      ~ as.numeric(ifelse(as.character(.) == \"&lt;1\", \"0\", as.character(.)))\n    )\n  )\n\n# 1.3 Check date range and interval\nrange(web_trends$date)\nunique(diff(web_trends$date))"
  },
  {
    "objectID": "assignment2.html#plot-website-data-interest-over-time",
    "href": "assignment2.html#plot-website-data-interest-over-time",
    "title": "Assignment 2 ‚Äì Google Trends",
    "section": "Plot Website Data (Interest Over Time)",
    "text": "Plot Website Data (Interest Over Time)\n\nweb_trends_long &lt;- web_trends |&gt;\n  pivot_longer(\n    cols = -date,\n    names_to = \"keyword\",\n    values_to = \"hits\"\n  )\n\nggplot(web_trends_long, aes(x = date, y = hits, color = keyword)) +\n  geom_line() +\n  labs(\n    title = \"Google Trends (website download): Interest over time\",\n    x = \"Date\",\n    y = \"Search interest (0‚Äì100)\"\n  )\n\n\nlibrary(gtrendsR)"
  },
  {
    "objectID": "assignment2.html#get-the-same-queries-via-gtrendsr",
    "href": "assignment2.html#get-the-same-queries-via-gtrendsr",
    "title": "Assignment 2 ‚Äì Google Trends",
    "section": "2.1 Get the same queries via gtrendsR",
    "text": "2.1 Get the same queries via gtrendsR\n\ngt &lt;- gtrends(\nkeyword = c(\"Trump\", \"Kamala Harris\", \"Election\"),\ngeo     = \"US\",          # match the country you used on the website\ntime    = \"today 12-m\"   # or \"2023-01-01 2024-01-01\", etc.\n)"
  },
  {
    "objectID": "assignment2.html#plot-the-gtrendsr-data",
    "href": "assignment2.html#plot-the-gtrendsr-data",
    "title": "Assignment 2 ‚Äì Google Trends",
    "section": "Plot the gtrendsR data",
    "text": "Plot the gtrendsR data\n\nggplot(iot, aes(x = date, y = hits, color = keyword)) +\ngeom_line() +\nlabs(\ntitle = \"Google Trends via gtrendsR: Interest over time\",\nx = \"Date\",\ny = \"Search interest (0‚Äì100)\"\n)"
  },
  {
    "objectID": "assignment2.html#save-to-csv-and-r-formats",
    "href": "assignment2.html#save-to-csv-and-r-formats",
    "title": "Assignment 2 ‚Äì Google Trends",
    "section": "Save to CSV and R formats",
    "text": "Save to CSV and R formats\n\n# Website data\n\nwrite_csv(web_trends, \"data/web_trends_trump_kamala_election_clean.csv\")\nsaveRDS(web_trends, \"data/web_trends_trump_kamala_election.rds\")\n\n# gtrendsR data\n\nwrite_csv(iot, \"data/gtrends_trump_kamala_election.csv\")\nsaveRDS(iot, \"data/gtrends_trump_kamala_election.rds\")"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "In this assignment, I use ACS 2023 5-year data to map median household income and summarize poverty for Texas counties.\n\n# Load packages\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(knitr)\n\noptions(tigris_use_cache = TRUE)\n\n# Set your Census API key for this R session\n# (replace \"YOUR_KEY_HERE\" with your actual key)\ncensus_api_key(\"52fd7a1a4a82921f1fd9dd3db0c338d2351c5ee1\", install = FALSE)"
  },
  {
    "objectID": "assignment3.html#objective",
    "href": "assignment3.html#objective",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "In this assignment, I use ACS 2023 5-year data to map median household income and summarize poverty for Texas counties.\n\n# Load packages\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(knitr)\n\noptions(tigris_use_cache = TRUE)\n\n# Set your Census API key for this R session\n# (replace \"YOUR_KEY_HERE\" with your actual key)\ncensus_api_key(\"52fd7a1a4a82921f1fd9dd3db0c338d2351c5ee1\", install = FALSE)"
  },
  {
    "objectID": "assignment3.html#choose-geography-and-variables",
    "href": "assignment3.html#choose-geography-and-variables",
    "title": "Assignment 3: Mapping Census Data",
    "section": "Choose geography and variables",
    "text": "Choose geography and variables\n\n# Texas state abbreviation\nstate_tx &lt;- \"TX\"\n\n# Named vector of variables\nacs_vars &lt;- c(\n  med_income = \"B19013_001\",  # Median household income\n  poverty    = \"B17001_002\"   # People below poverty level\n)"
  },
  {
    "objectID": "assignment3.html#download-acs-2023-5-year-data-with-geometry",
    "href": "assignment3.html#download-acs-2023-5-year-data-with-geometry",
    "title": "Assignment 3: Mapping Census Data",
    "section": "Download ACS 2023 5-year data (with geometry)",
    "text": "Download ACS 2023 5-year data (with geometry)\n\ntx_acs &lt;- get_acs(\n  geography = \"county\",\n  state     = state_tx,\n  variables = acs_vars,\n  year      = 2023,\n  survey    = \"acs5\",\n  geometry  = TRUE,\n  output    = \"wide\"   # wide gives med_incomeE, med_incomeM, etc.\n)\n\n\ndplyr::glimpse(tx_acs)\n\nRows: 254\nColumns: 7\n$ GEOID       &lt;chr&gt; \"48355\", \"48215\", \"48061\", \"48479\", \"48057\", \"48323\", \"484‚Ä¶\n$ NAME        &lt;chr&gt; \"Nueces County, Texas\", \"Hidalgo County, Texas\", \"Cameron ‚Ä¶\n$ med_incomeE &lt;dbl&gt; 66021, 52281, 51334, 62506, 71870, 51270, 59673, 95155, 39‚Ä¶\n$ med_incomeM &lt;dbl&gt; 1570, 1265, 1640, 2316, 10785, 5723, 4312, 2480, 17125, 28‚Ä¶\n$ povertyE    &lt;dbl&gt; 61023, 237121, 102583, 55716, 1717, 12981, 8377, 27057, 49‚Ä¶\n$ povertyM    &lt;dbl&gt; 3709, 8739, 4345, 4052, 468, 1758, 1316, 2210, 259, 1500, ‚Ä¶\n$ geometry    &lt;MULTIPOLYGON [¬∞]&gt; MULTIPOLYGON (((-97.11172 2..., MULTIPOLYGON ‚Ä¶"
  },
  {
    "objectID": "assignment3.html#map-median-household-income-by-county",
    "href": "assignment3.html#map-median-household-income-by-county",
    "title": "Assignment 3: Mapping Census Data",
    "section": "Map: median household income by county",
    "text": "Map: median household income by county\n\nggplot(tx_acs) +\n  geom_sf(aes(fill = med_incomeE), color = NA) +\n  scale_fill_viridis_c(\n    option = \"plasma\",\n    labels = scales::label_dollar(prefix = \"$\")\n  ) +\n  labs(\n    title = \"ACS 2023 5-year: Median Household Income ‚Äî Texas Counties\",\n    fill  = \"Median HH Income\"\n  ) +\n  theme_minimal()\n\n\n\n\nACS 2023 5-year: Median household income by county in Texas\n\n\n\n\nThe map shows that counties around large metros such as Dallas‚ÄìFort Worth, Austin, and Houston tend to have higher median incomes, while many rural counties in West and South Texas have lower incomes."
  },
  {
    "objectID": "assignment3.html#table-top-and-bottom-10-counties-by-people-below-poverty",
    "href": "assignment3.html#table-top-and-bottom-10-counties-by-people-below-poverty",
    "title": "Assignment 3: Mapping Census Data",
    "section": "Table: top and bottom 10 counties by people below poverty",
    "text": "Table: top and bottom 10 counties by people below poverty\ntx_poverty_tbl &lt;- tx_acs |&gt;\n  sf::st_drop_geometry() |&gt;\n  dplyr::select(\n    county      = NAME,\n    povertyE,\n    povertyM,\n    med_incomeE,\n    med_incomeM\n  )\n\n# Top 10 counties by number of people below poverty\ntop10_poverty &lt;- tx_poverty_tbl |&gt;\n  dplyr::arrange(dplyr::desc(povertyE)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::mutate(rank_group = \"Highest poverty (count)\")\n\n# Bottom 10 counties by number of people below poverty\nbottom10_poverty &lt;- tx_poverty_tbl |&gt;\n  dplyr::arrange(povertyE) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::mutate(rank_group = \"Lowest poverty (count)\")\n\ncombined_tbl &lt;- dplyr::bind_rows(top10_poverty, bottom10_poverty) |&gt;\n  dplyr::select(\n    rank_group,\n    county,\n    povertyE, povertyM,\n    med_incomeE, med_incomeM\n  )\n\nkable(\n  combined_tbl,\n  caption = \"Top and bottom 10 Texas counties by number of people below poverty (ACS 2023 5-year).  E = Estimate, M = Margin of Error.\"\n)\n\nTop and bottom 10 Texas counties by number of people below poverty (ACS 2023 5-year). E = Estimate, M = Margin of Error.\n\n\n\n\n\n\n\n\n\n\nrank_group\ncounty\npovertyE\npovertyM\nmed_incomeE\nmed_incomeM\n\n\n\n\nHighest poverty (count)\nHarris County, Texas\n749481\n15891\n73104\n650\n\n\nHighest poverty (count)\nDallas County, Texas\n359950\n10475\n74149\n771\n\n\nHighest poverty (count)\nBexar County, Texas\n294002\n8323\n70571\n776\n\n\nHighest poverty (count)\nHidalgo County, Texas\n237121\n8739\n52281\n1265\n\n\nHighest poverty (count)\nTarrant County, Texas\n229884\n7849\n81905\n930\n\n\nHighest poverty (count)\nEl Paso County, Texas\n160998\n7148\n58859\n1208\n\n\nHighest poverty (count)\nTravis County, Texas\n140926\n6636\n97169\n1289\n\n\nHighest poverty (count)\nCameron County, Texas\n102583\n4345\n51334\n1640\n\n\nHighest poverty (count)\nCollin County, Texas\n69846\n4614\n117588\n1528\n\n\nHighest poverty (count)\nDenton County, Texas\n65649\n4233\n108185\n1464\n\n\nLowest poverty (count)\nKenedy County, Texas\n3\n5\nNA\nNA\n\n\nLowest poverty (count)\nLoving County, Texas\n5\n7\n51087\n613\n\n\nLowest poverty (count)\nKing County, Texas\n29\n26\n70192\n28406\n\n\nLowest poverty (count)\nBorden County, Texas\n35\n28\n64250\n20041\n\n\nLowest poverty (count)\nSterling County, Texas\n37\n25\n78750\n69746\n\n\nLowest poverty (count)\nRoberts County, Texas\n50\n35\n66118\n8220\n\n\nLowest poverty (count)\nKent County, Texas\n56\n32\n71420\n4616\n\n\nLowest poverty (count)\nMcMullen County, Texas\n57\n47\n45833\n25091\n\n\nLowest poverty (count)\nTerrell County, Texas\n70\n46\n46989\n12811\n\n\nLowest poverty (count)\nGlasscock County, Texas\n91\n67\n106806\n46346\n\n\n\nThe highest-poverty counties by count are mostly large, population-dense counties, while the lowest-poverty counties are small counties with few residents. Looking at poverty counts together with median income highlights how both population size and income levels shape the distribution of poverty across Texas."
  },
  {
    "objectID": "assignment6.html",
    "href": "assignment6.html",
    "title": "Assignment 6 ‚Äì Text Analytics using quanteda",
    "section": "",
    "text": "The quanteda package is a comprehensive framework for quantitative text analysis in R. It provides tools for importing text, cleaning it, tokenizing it, and converting it into document‚Äìfeature matrices (DFMs). From these DFMs, users can apply advanced text-analysis methods such as word clouds, keyness comparison, topic modeling, co-occurrence networks, and scaling models such as LSA, Wordfish, and Wordscores. Compared to base R text processing, quanteda makes it faster and more efficient to process large text collections and run reproducible analyses."
  },
  {
    "objectID": "assignment6.html#reading-about-the-quanteda-package",
    "href": "assignment6.html#reading-about-the-quanteda-package",
    "title": "Assignment 6 ‚Äì Text Analytics using quanteda",
    "section": "",
    "text": "The quanteda package is a comprehensive framework for quantitative text analysis in R. It provides tools for importing text, cleaning it, tokenizing it, and converting it into document‚Äìfeature matrices (DFMs). From these DFMs, users can apply advanced text-analysis methods such as word clouds, keyness comparison, topic modeling, co-occurrence networks, and scaling models such as LSA, Wordfish, and Wordscores. Compared to base R text processing, quanteda makes it faster and more efficient to process large text collections and run reproducible analyses."
  },
  {
    "objectID": "assignment6.html#bidenxi-summit-tweets",
    "href": "assignment6.html#bidenxi-summit-tweets",
    "title": "Assignment 6 ‚Äì Text Analytics using quanteda",
    "section": "2. Biden‚ÄìXi Summit Tweets",
    "text": "2. Biden‚ÄìXi Summit Tweets\nThis section uses the code in quanteda_textanalytics01.R.\nThe script loads tweets about the November 2021 Biden‚ÄìXi virtual summit, builds DFMs, performs latent semantic analysis (LSA), and plots hashtag and user networks.\n\n2.1 Run the script\n\nsource(\"quanteda_textanalytics01.R\")\n\nPackage version: 4.3.1\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nRows: 14520 Columns: 90\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2.2 Interpretation\n\nLSA (Latent Semantic Analysis)\nThe LSA output shows that tweets cluster into several thematic groups. Based on the most frequent terms in each dimension, the discussions appear to revolve around:\n\nDiplomacy & cooperation (summit, meeting, leaders, virtual talks)\n\nGeopolitical tension (China, security, human rights, Taiwan)\n\nU.S. domestic reactions (journalists, think-tank commentary)\n\nThese clusters reflect how Twitter conversations typically separate into news reporting, political opinion, and geopolitical analysis.\n\n\nHashtag Network Plot\n\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nThe hashtag network reveals groups of frequently co-occurring hashtags. For example:\n\nNews-related tags (#Breaking, #US, #China)\n\nPolitical commentary tags (#Biden, #XiJinping)\n\nIssue-specific tags (e.g., related to trade, security, diplomacy)\n\nHashtags that appear close together represent shared topics in the discourse.\n\n\nUser Mention Network Plot\n\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\",\n                 edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nThis network shows which accounts dominate the conversation. Central users typically include:\n\nJournalists reporting on the summit\n\nNews outlets\n\nPolitical commentators\n\nAnalysts covering U.S.‚ÄìChina relations\n\nThe high degree of clustering indicates that a small number of influential accounts shape much of the discussion."
  },
  {
    "objectID": "assignment6.html#u.s.-presidential-inaugural-speeches",
    "href": "assignment6.html#u.s.-presidential-inaugural-speeches",
    "title": "Assignment 6 ‚Äì Text Analytics using quanteda",
    "section": "3. U.S. Presidential Inaugural Speeches",
    "text": "3. U.S. Presidential Inaugural Speeches\nThis section uses the code in quanteda_textanalytics02.R.\nThe script analyzes the U.S. inaugural address corpus using wordclouds, keyword-in-context (KWIC), frequency charts, and text scaling.\n\n3.1 Run the script\n\nsource(\"quanteda_textanalytics02.R\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Interpretation\n\nEarly Inaugural Speech Wordclouds\nThe earliest speeches (1789‚Äì1826) emphasize terms such as government, duty, public, union, and people.\nThis reflects the period‚Äôs preoccupation with defining governmental roles and stabilizing the new republic.\n\n\nComparing Bush, Obama, and Trump Wordclouds\nThe comparison wordcloud shows clear stylistic differences:\n\nBush: terms relating to security, freedom, resolve\n\nObama: focus on hope, unity, responsibility\n\nTrump: ‚ÄúAmerica-first‚Äù framing, strong emphasis on national identity\n\nThese terms highlight ideological and rhetorical distinctions between administrations.\n\n\nKWIC + X-Ray Plots\nKWIC searches for words such as ‚Äúamerican‚Äù, ‚Äúpeople‚Äù, and ‚Äúcommunist‚Äù reveal how each term‚Äôs use varies by president.\nThe x-ray plot shows that:\n\n‚ÄúAmerican‚Äù appears more frequently in later decades\n\n‚ÄúPeople‚Äù remains a consistently common term\n\n‚ÄúCommunist‚Äù spikes during Cold War speeches\n\n\n\nFrequency Plot for ‚Äúamerican‚Äù\nThis plot shows the rising use of the term ‚Äúamerican‚Äù over time.\nIt suggests a growing rhetorical emphasis on national identity throughout the 20th and 21st centuries.\n\n\nKeyness Analysis: Trump vs.¬†Obama\nKeyness identifies words most strongly associated with one president versus the other.\n\nWords with positive keyness ‚Üí more common in Trump‚Äôs speech\n\nWords with negative keyness ‚Üí more common in Obama‚Äôs speech\n\nThis captures stylistic and thematic contrasts between the two presidents."
  },
  {
    "objectID": "assignment6.html#what-is-wordfish",
    "href": "assignment6.html#what-is-wordfish",
    "title": "Assignment 6 ‚Äì Text Analytics using quanteda",
    "section": "4. What is Wordfish?",
    "text": "4. What is Wordfish?\nWordfish is an unsupervised text scaling model that estimates:\n\nA latent dimension (e.g., ideology)\nWord discrimination values\nDocument positions on the latent scale\n\nIt uses a Poisson model of word counts:\n\nWords that strongly indicate one side of the scale receive high discrimination values\n\nDocuments containing those words more frequently get shifted toward that side of the latent dimension\n\nThe model is often used to measure:\n\nParty ideology\n\nPolicy stance\n\nRhetorical positioning\n\nIn this assignment‚Äôs script, Wordfish is demonstrated using Irish budget speeches, showing how parties and years align on an inferred economic policy dimension."
  },
  {
    "objectID": "assignment6.html#summary",
    "href": "assignment6.html#summary",
    "title": "Assignment 6 ‚Äì Text Analytics using quanteda",
    "section": "Summary",
    "text": "Summary\nThis assignment demonstrated how quanteda enables:\n\nLarge-scale text preprocessing\n\nExploratory visualizations (wordclouds, networks, frequency plots)\n\nAdvanced modeling (LSA, keyness, Wordfish)\n\nInterpretation of political and social communication via text analytics"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4 ‚Äì Webscraping 1",
    "section": "",
    "text": "In this assignment I use rvest to scrape the table of foreign-exchange reserves from Wikipedia and then clean the resulting data frame.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.6\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(stringr)\nlibrary(lubridate)\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves\"\n\nwikiforreserve &lt;- read_html(url)"
  },
  {
    "objectID": "assignment4.html#scraping-wikipedia-foreign-reserve-data",
    "href": "assignment4.html#scraping-wikipedia-foreign-reserve-data",
    "title": "Assignment 4 ‚Äì Webscraping 1",
    "section": "",
    "text": "In this assignment I use rvest to scrape the table of foreign-exchange reserves from Wikipedia and then clean the resulting data frame.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.6\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(stringr)\nlibrary(lubridate)\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves\"\n\nwikiforreserve &lt;- read_html(url)"
  },
  {
    "objectID": "assignment4.html#extract-the-first-table-country-level-reserves",
    "href": "assignment4.html#extract-the-first-table-country-level-reserves",
    "title": "Assignment 4 ‚Äì Webscraping 1",
    "section": "1.1 Extract the first table (country-level reserves)",
    "text": "1.1 Extract the first table (country-level reserves)\n\n# get all tables from the main content area\nall_tables &lt;- wikiforreserve |&gt;\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table')\n\n# first table = country-level reserves\nforeignreserve &lt;- all_tables[[1]] |&gt;\n  html_table(fill = TRUE)\n\n# keep only the first 6 columns (the main country table)\nfores &lt;- foreignreserve[, 1:6]\n\n# give those 6 columns clear names\nnames(fores) &lt;- c(\"Rank\", \"Country\", \"Forexres\", \"Date\", \"Change\", \"Sources\")\n\nhead(fores)\n\n# A tibble: 6 √ó 6\n  Rank                               Country     Forexres   Date  Change Sources\n  &lt;chr&gt;                              &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 Country(as recognized by the U.N.) Continent   Including‚Ä¶ Incl‚Ä¶ Exclu‚Ä¶ Exclud‚Ä¶\n2 Country(as recognized by the U.N.) Continent   millions ‚Ä¶ Chan‚Ä¶ milli‚Ä¶ Change \n3 China                              Asia        3,643,149  41,0‚Ä¶ 3,389‚Ä¶ 31,221 \n4 Japan                              Asia        1,324,210  19,7‚Ä¶ 1,230‚Ä¶ 16,230 \n5 Switzerland                        Europe      1,007,710  13,9‚Ä¶ 897,2‚Ä¶ 14,490 \n6 Russia                             Europe/Asia 734,100    14,3‚Ä¶ 434,4‚Ä¶ 1,517"
  },
  {
    "objectID": "assignment4.html#cleaning-the-data-frame",
    "href": "assignment4.html#cleaning-the-data-frame",
    "title": "Assignment 4 ‚Äì Webscraping 1",
    "section": "2. Cleaning the Data Frame",
    "text": "2. Cleaning the Data Frame\n\n2.1 Clean the Date variable\n\nfores &lt;- fores |&gt;\n  mutate(\n    Date_clean_chr = str_split_fixed(Date, \"\\\\[\", n = 2)[, 1] |&gt; str_trim(),\n    Date_clean = dmy(Date_clean_chr)\n  )\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `Date_clean = dmy(Date_clean_chr)`.\nCaused by warning:\n! All formats failed to parse. No formats found.\n\nfores |&gt;\n  select(Country, Date, Date_clean_chr, Date_clean) |&gt;\n  head()\n\n# A tibble: 6 √ó 4\n  Country     Date           Date_clean_chr Date_clean\n  &lt;chr&gt;       &lt;chr&gt;          &lt;chr&gt;          &lt;date&gt;    \n1 Continent   Including gold Including gold NA        \n2 Continent   Change         Change         NA        \n3 Asia        41,079         41,079         NA        \n4 Asia        19,774         19,774         NA        \n5 Europe      13,935         13,935         NA        \n6 Europe/Asia 14,300         14,300         NA        \n\n\n\n\n2.2 Remove unneeded rows and clean numeric fields\n\nfores_clean &lt;- fores |&gt;\n  mutate(\n    Forexres_usd = parse_number(Forexres),\n    Change_num   = parse_number(Change)\n  ) |&gt;\n  select(\n    Country,\n    Forexres_usd,\n    Date = Date_clean,\n    Change = Change_num\n  )\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\n‚Ñπ In argument: `Forexres_usd = parse_number(Forexres)`.\nCaused by warning:\n! 2 parsing failures.\nrow col expected         actual\n  1  -- a number Including gold\n  2  -- a number millions U.S.$\n‚Ñπ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nhead(fores_clean, 10)\n\n# A tibble: 10 √ó 4\n   Country     Forexres_usd Date    Change\n   &lt;chr&gt;              &lt;dbl&gt; &lt;date&gt;   &lt;dbl&gt;\n 1 Continent             NA NA          NA\n 2 Continent             NA NA          NA\n 3 Asia             3643149 NA     3389306\n 4 Asia             1324210 NA     1230940\n 5 Europe           1007710 NA      897295\n 6 Europe/Asia       734100 NA      434487\n 7 Asia              692576 NA      585719\n 8 Asia              597430 NA      544300\n 9 Asia              434547 NA      434116\n10 Asia              421400 NA      416216"
  },
  {
    "objectID": "assignment4.html#modifying-the-program-to-scrape-other-tables",
    "href": "assignment4.html#modifying-the-program-to-scrape-other-tables",
    "title": "Assignment 4 ‚Äì Webscraping 1",
    "section": "3. Modifying the Program to Scrape Other Tables",
    "text": "3. Modifying the Program to Scrape Other Tables\n\nall_tables_df &lt;- wikiforreserve |&gt;\n  html_table(fill = TRUE)\n\nlength(all_tables_df)\n\n[1] 4\n\n\n\ntable2 &lt;- all_tables_df[[2]]\ntable3 &lt;- all_tables_df[[3]]\n\nhead(table2)\n\n# A tibble: 6 √ó 15\n  ``                      `` ``    Currency composition‚Ä¶¬π Currency composition‚Ä¶¬≤\n  &lt;chr&gt;                &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;                 \n1 \"\"                      NA \"\"    USD                    EUR                   \n2 \"@supports(writing-‚Ä¶  2019 \"Q1\"  6,727.09               2,208.79              \n3 \"@supports(writing-‚Ä¶  2019 \"Q2\"  6,752.28               2,264.88              \n4 \"@supports(writing-‚Ä¶  2019 \"Q3\"  6,728.85               2,212.74              \n5 \"@supports(writing-‚Ä¶  2019 \"Q4\"  6,674.83               2,279.30              \n6 \"@supports(writing-‚Ä¶  2020 \"Q1\"  6,794.91               2,197.30              \n# ‚Ñπ abbreviated names:\n#   ¬π‚Äã`Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]`,\n#   ¬≤‚Äã`Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]`\n# ‚Ñπ 10 more variables:\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]` &lt;chr&gt;,\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]` &lt;chr&gt;,\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]` &lt;chr&gt;, ‚Ä¶\n\nhead(table3)\n\n# A tibble: 4 √ó 2\n  .mw-parser-output .navbar{display:inline;font-size:88‚Ä¶¬π .mw-parser-output .n‚Ä¶¬≤\n  &lt;chr&gt;                                                   &lt;chr&gt;                 \n1 Global                                                  \"Bank for Internation‚Ä¶\n2 Policies                                                \"Basel Accords\\nCapit‚Ä¶\n3 Bretton Woodssystem                                     \"International Moneta‚Ä¶\n4 Lists                                                   \"List of central bank‚Ä¶\n# ‚Ñπ abbreviated names:\n#   ¬π‚Äã`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteCentral banks`,\n#   ¬≤‚Äã`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteCentral banks`\n\n\n\nget_reserve_table &lt;- function(idx) {\n  tab &lt;- all_tables_df[[idx]]\n  tab\n}\n\nexample_table &lt;- get_reserve_table(2)\nhead(example_table)\n\n# A tibble: 6 √ó 15\n  ``                      `` ``    Currency composition‚Ä¶¬π Currency composition‚Ä¶¬≤\n  &lt;chr&gt;                &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;                 \n1 \"\"                      NA \"\"    USD                    EUR                   \n2 \"@supports(writing-‚Ä¶  2019 \"Q1\"  6,727.09               2,208.79              \n3 \"@supports(writing-‚Ä¶  2019 \"Q2\"  6,752.28               2,264.88              \n4 \"@supports(writing-‚Ä¶  2019 \"Q3\"  6,728.85               2,212.74              \n5 \"@supports(writing-‚Ä¶  2019 \"Q4\"  6,674.83               2,279.30              \n6 \"@supports(writing-‚Ä¶  2020 \"Q1\"  6,794.91               2,197.30              \n# ‚Ñπ abbreviated names:\n#   ¬π‚Äã`Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]`,\n#   ¬≤‚Äã`Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]`\n# ‚Ñπ 10 more variables:\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]` &lt;chr&gt;,\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]` &lt;chr&gt;,\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[213]` &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "assignment4.html#data-plan-for-acquiring-web-data-for-research",
    "href": "assignment4.html#data-plan-for-acquiring-web-data-for-research",
    "title": "Assignment 4 ‚Äì Webscraping 1",
    "section": "4. Data Plan for Acquiring Web Data for Research",
    "text": "4. Data Plan for Acquiring Web Data for Research\n\n4.1 Define the research question\nIdentify variables, scope, and intended analysis.\n\n\n4.2 Identify data sources\nPrefer official sources (IMF, World Bank, OECD).\n\n\n4.3 Choose collection method\nrvest for tables, httr/jsonlite for APIs.\n\n\n4.4 Cleaning workflow\nStandardize names, convert numeric/date fields, remove footnotes.\n\n\n4.5 Storage & documentation\nUse data/raw and data/clean, maintain a data dictionary.\n\n\n4.6 Update strategy\nRe-scrape periodically and check layout changes.\n\n\n4.7 Ethics\nRespect robots.txt and cite data sources properly."
  },
  {
    "objectID": "assignment4.html#summary",
    "href": "assignment4.html#summary",
    "title": "Assignment 4 ‚Äì Webscraping 1",
    "section": "5. Summary",
    "text": "5. Summary\nThis assignment involved scraping with rvest, cleaning variables, extracting multiple tables, and designing a research data acquisition plan."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "anugrahasamuel.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1 + 1\n\n[1] 3"
  }
]